{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from src.utils import load_prompt\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    ")\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "load_dotenv(\"src/.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils\n",
    "def load_text(path):\n",
    "    with open(path, \"r\") as fp:\n",
    "        return fp.read()\n",
    "\n",
    "def load_prompt(prompt):\n",
    "    return load_text(f\"prompts/{prompt}.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Onboarding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "onboarding_prompt = load_prompt(\"onboarding\")\n",
    "human_prompt_template = \"{technology_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message_prompt = SystemMessagePromptTemplate.from_template(onboarding_prompt)\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_prompt_template)\n",
    "\n",
    "onboarding_chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [system_message_prompt, human_message_prompt]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "onboarding = chat(\n",
    "    onboarding_chat_prompt.format_prompt(\n",
    "        technology_name=\"Apache Beam\"\n",
    "    ).to_messages()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Onboarding\n",
       "\n",
       "### What problem does this aim to solve?\n",
       "\n",
       "Apache Beam addresses the challenge of building and executing data processing pipelines that are scalable, portable, and expressive. In the world of big data, processing large volumes of data efficiently and reliably is a complex task. Traditional approaches often involve writing custom code for each data processing job, leading to code duplication, maintenance challenges, and limited scalability. Apache Beam solves these problems by providing a unified programming model and a set of APIs that enable developers to write data processing pipelines that can run on various execution engines, such as Apache Flink, Apache Spark, and Google Cloud Dataflow.\n",
       "\n",
       "### What sub-category of technologies is this?\n",
       "\n",
       "Apache Beam falls under the sub-category of \"data processing frameworks\" within the broader field of big data and distributed computing. It is a tool that simplifies the development and execution of data processing pipelines, allowing developers to focus on the logic of their data transformations rather than the underlying infrastructure. Apache Beam's portable and expressive nature makes it suitable for a wide range of use cases, including batch and stream processing, ETL (Extract, Transform, Load) pipelines, and real-time analytics."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(onboarding.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Developer life with/without the tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Developer life with/without this tool\n",
       "\n",
       "### Without Kubernetes\n",
       "\n",
       "#### Manual Deployment and Scaling\n",
       "\n",
       "Developers are responsible for manually deploying and scaling applications on individual servers or virtual machines.\n",
       "This process involves configuring and managing each server individually, which can be time-consuming and error-prone.\n",
       "\n",
       "#### Resource Management\n",
       "\n",
       "Without Kubernetes, developers need to manually allocate and manage resources for each application.\n",
       "This includes monitoring resource usage, optimizing resource allocation, and ensuring efficient utilization.\n",
       "\n",
       "#### High Availability and Fault Tolerance\n",
       "\n",
       "Ensuring high availability and fault tolerance requires manual setup and configuration of load balancers, failover mechanisms, and redundancy.\n",
       "This can be complex and time-consuming, especially in large-scale deployments.\n",
       "\n",
       "#### Example Scenario\n",
       "\n",
       "A developer needs to deploy a microservices-based application on multiple servers, manage resource allocation, and ensure high availability.\n",
       "This involves manually configuring load balancers, monitoring resource usage, and handling failover scenarios.\n",
       "\n",
       "### With Kubernetes\n",
       "\n",
       "#### Automated Deployment and Scaling\n",
       "\n",
       "Kubernetes automates the deployment and scaling of applications using containerization.\n",
       "Developers define the desired state of the application using YAML or JSON files, and Kubernetes takes care of the rest.\n",
       "\n",
       "Example Deployment YAML:\n",
       "\n",
       "```yaml\n",
       "apiVersion: apps/v1\n",
       "kind: Deployment\n",
       "metadata:\n",
       "  name: my-app\n",
       "spec:\n",
       "  replicas: 3\n",
       "  selector:\n",
       "    matchLabels:\n",
       "      app: my-app\n",
       "  template:\n",
       "    metadata:\n",
       "      labels:\n",
       "        app: my-app\n",
       "    spec:\n",
       "      containers:\n",
       "      - name: my-app\n",
       "        image: my-app:latest\n",
       "        ports:\n",
       "        - containerPort: 8080\n",
       "```\n",
       "\n",
       "#### Efficient Resource Management\n",
       "\n",
       "Kubernetes automatically manages resource allocation based on the defined requirements and constraints.\n",
       "It optimizes resource utilization by dynamically scaling resources up or down based on demand.\n",
       "\n",
       "#### High Availability and Fault Tolerance\n",
       "\n",
       "Kubernetes provides built-in mechanisms for high availability and fault tolerance.\n",
       "It automatically handles load balancing, failover, and replication of application instances across multiple nodes.\n",
       "\n",
       "#### Example Workflow\n",
       "\n",
       "A developer defines the desired state of the application using a Kubernetes deployment file (`kubectl apply -f deployment.yaml`).\n",
       "Kubernetes automatically deploys the application, manages resource allocation, and ensures high availability.\n",
       "Scaling the application can be done by updating the deployment file (`kubectl apply -f deployment.yaml`) or using commands like `kubectl scale`.\n",
       "\n",
       "Overall, Kubernetes simplifies the deployment, scaling, resource management, and high availability of applications, allowing developers to focus on writing code rather than managing infrastructure."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with_and_without_prompt = load_prompt(\"with-and-without\")\n",
    "human_prompt_template = \"{technology_name}\"\n",
    "\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(with_and_without_prompt)\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_prompt_template)\n",
    "\n",
    "with_and_without_chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [system_message_prompt, human_message_prompt]\n",
    ")\n",
    "\n",
    "with_and_without = chat(\n",
    "    with_and_without_chat_prompt.format_prompt(\n",
    "        technology_name=\"Kubernetes\"\n",
    "    ).to_messages()\n",
    ")\n",
    "\n",
    "\n",
    "display(Markdown(with_and_without.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Core Concepts\n",
       "\n",
       "### Data Processing Pipelines\n",
       "Apache Beam is a technology that enables the development and execution of data processing pipelines. A data processing pipeline is a sequence of steps that transform and analyze data. It allows you to define the flow of data from source to destination, applying various operations and transformations along the way.\n",
       "\n",
       "### Unified Programming Model\n",
       "Apache Beam provides a unified programming model that allows you to write data processing pipelines in a language-agnostic manner. This means that you can write your pipelines using one of the supported programming languages (such as Java, Python, or Go) and execute them on different execution engines (such as Apache Flink, Apache Spark, or Google Cloud Dataflow) without modifying the code.\n",
       "\n",
       "### PCollection\n",
       "In Apache Beam, a PCollection (short for \"processing collection\") represents a collection of data elements that are processed as part of a pipeline. It can be thought of as an abstraction for a distributed data set. PCollections can be created from various data sources, such as files, databases, or message queues, and can be transformed using operations like filtering, mapping, or aggregating.\n",
       "\n",
       "### Transformations\n",
       "Transformations are the building blocks of Apache Beam pipelines. They define the operations that are applied to PCollections to produce new PCollections. Transformations can be simple, such as filtering or mapping individual elements, or they can be complex, involving aggregations or joining multiple PCollections together. Apache Beam provides a rich set of built-in transformations, and you can also create custom transformations to suit your specific needs.\n",
       "\n",
       "### Windowing\n",
       "Windowing is a concept in Apache Beam that allows you to divide the data in a PCollection into logical windows based on time or other criteria. This is useful when dealing with streaming data or when you want to perform computations over fixed time intervals or sliding windows. Windowing enables you to apply operations like aggregations or sessionization on data within each window, providing more flexibility in data processing."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "core_concepts_prompt = load_prompt(\"core-concepts\")\n",
    "human_prompt_template = \"{technology_name}\"\n",
    "\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(core_concepts_prompt)\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_prompt_template)\n",
    "\n",
    "core_concepts_chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [system_message_prompt, human_message_prompt]\n",
    ")\n",
    "\n",
    "core_concepts = chat(\n",
    "    core_concepts_chat_prompt.format_prompt(\n",
    "        technology_name=\"Apache Beam\"\n",
    "    ).to_messages()\n",
    ")\n",
    "\n",
    "\n",
    "display(Markdown(core_concepts.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Core APIs\n",
       "\n",
       "### `kubectl create`\n",
       "\n",
       "- Purpose: Creates a resource in the Kubernetes cluster.\n",
       "- Usage Example:\n",
       "\n",
       "```bash\n",
       "kubectl create deployment my-app --image=my-image:latest\n",
       "```\n",
       "\n",
       "### `kubectl apply`\n",
       "\n",
       "- Purpose: Applies a configuration to the Kubernetes cluster, creating or updating resources.\n",
       "- Usage Example:\n",
       "\n",
       "```bash\n",
       "kubectl apply -f my-config.yaml\n",
       "```\n",
       "\n",
       "### `kubectl get`\n",
       "\n",
       "- Purpose: Retrieves information about resources in the Kubernetes cluster.\n",
       "- Usage Example:\n",
       "\n",
       "```bash\n",
       "kubectl get pods\n",
       "```\n",
       "\n",
       "### `kubectl describe`\n",
       "\n",
       "- Purpose: Provides detailed information about a specific resource in the Kubernetes cluster.\n",
       "- Usage Example:\n",
       "\n",
       "```bash\n",
       "kubectl describe pod my-pod\n",
       "```\n",
       "\n",
       "### `kubectl delete`\n",
       "\n",
       "- Purpose: Deletes a resource from the Kubernetes cluster.\n",
       "- Usage Example:\n",
       "\n",
       "```bash\n",
       "kubectl delete deployment my-app\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "core_apis_prompt = load_prompt(\"core-apis\")\n",
    "human_prompt_template = \"{technology_name}\"\n",
    "\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(core_apis_prompt)\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_prompt_template)\n",
    "\n",
    "core_apis_chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [system_message_prompt, human_message_prompt]\n",
    ")\n",
    "\n",
    "core_apis = chat(\n",
    "    core_apis_chat_prompt.format_prompt(\n",
    "        technology_name=\"Kubernetes\"\n",
    "    ).to_messages()\n",
    ")\n",
    "\n",
    "\n",
    "display(Markdown(core_apis.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Core APIs\n",
      "\n",
      "### `kubectl create`\n",
      "\n",
      "- Purpose: Creates a resource in the Kubernetes cluster.\n",
      "- Usage Example:\n",
      "\n",
      "```bash\n",
      "kubectl create deployment my-app --image=my-image:latest\n",
      "```\n",
      "\n",
      "### `kubectl apply`\n",
      "\n",
      "- Purpose: Applies a configuration to the Kubernetes cluster, creating or updating resources.\n",
      "- Usage Example:\n",
      "\n",
      "```bash\n",
      "kubectl apply -f my-config.yaml\n",
      "```\n",
      "\n",
      "### `kubectl get`\n",
      "\n",
      "- Purpose: Retrieves information about resources in the Kubernetes cluster.\n",
      "- Usage Example:\n",
      "\n",
      "```bash\n",
      "kubectl get pods\n",
      "```\n",
      "\n",
      "### `kubectl describe`\n",
      "\n",
      "- Purpose: Provides detailed information about a specific resource in the Kubernetes cluster.\n",
      "- Usage Example:\n",
      "\n",
      "```bash\n",
      "kubectl describe pod my-pod\n",
      "```\n",
      "\n",
      "### `kubectl delete`\n",
      "\n",
      "- Purpose: Deletes a resource from the Kubernetes cluster.\n",
      "- Usage Example:\n",
      "\n",
      "```bash\n",
      "kubectl delete deployment my-app\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(core_apis.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real Life Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from openai import OpenAI\n",
    "from tavily import TavilyClient\n",
    "\n",
    "# Initialize clients with API keys\n",
    "client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "tavily_client = TavilyClient(api_key=os.environ[\"TAVILY_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform a Tavily search\n",
    "def tavily_search(query):\n",
    "    search_result = tavily_client.get_search_context(query, search_depth=\"advanced\", max_tokens=8000, include_domains=[\"github.com\"])\n",
    "    return search_result\n",
    "\n",
    "# Function to wait for a run to complete\n",
    "def wait_for_run_completion(thread_id, run_id):\n",
    "    while True:\n",
    "        time.sleep(1)\n",
    "        run = client.beta.threads.runs.retrieve(thread_id=thread_id, run_id=run_id)\n",
    "        print(f\"Current run status: {run.status}\")\n",
    "        if run.status in ['completed', 'failed', 'requires_action']:\n",
    "            return run\n",
    "\n",
    "# Function to handle tool output submission\n",
    "def submit_tool_outputs(thread_id, run_id, tools_to_call):\n",
    "    tool_output_array = []\n",
    "    for tool in tools_to_call:\n",
    "        output = None\n",
    "        tool_call_id = tool.id\n",
    "        function_name = tool.function.name\n",
    "        function_args = tool.function.arguments\n",
    "\n",
    "        if function_name == \"tavily_search\":\n",
    "            output = tavily_search(query=json.loads(function_args)[\"query\"])\n",
    "\n",
    "        if output:\n",
    "            tool_output_array.append({\"tool_call_id\": tool_call_id, \"output\": output})\n",
    "\n",
    "    return client.beta.threads.runs.submit_tool_outputs(\n",
    "        thread_id=thread_id,\n",
    "        run_id=run_id,\n",
    "        tool_outputs=tool_output_array\n",
    "    )\n",
    "\n",
    "def get_assistant_response(thread_id: str):\n",
    "    messages = client.beta.threads.messages.list(thread_id=thread_id)\n",
    "    return messages.data[0].content[0].text.value\n",
    "\n",
    "\n",
    "def get_assistant(prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Returns OpenAI assistant that has instructions of `prompt`\n",
    "    \"\"\"\n",
    "    curr_assistant_id = os.environ[\"REAL_LIFE_EXAMPLES_OPENAI_ASSISTANT_ID\"]\n",
    "\n",
    "    curr_assistant = client.beta.assistants.retrieve(curr_assistant_id)\n",
    "\n",
    "    if curr_assistant.instructions == prompt:\n",
    "        return curr_assistant\n",
    "    \n",
    "    else:\n",
    "        print(\"Current assistant stored in ENV does not use the provided prompt. Making a new assistant with new prompt.\")\n",
    "    \n",
    "        # Create the assistant\n",
    "        assistant = client.beta.assistants.create(\n",
    "            instructions=prompt,\n",
    "            model=\"gpt-4-1106-preview\",\n",
    "            tools=[{\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": \"tavily_search\",\n",
    "                    \"description\": \"Get information on recent events from the web.\",\n",
    "                    \"parameters\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"query\": {\"type\": \"string\", \"description\": \"The search query to use. For example: 'Latest news on Nvidia stock performance'\"},\n",
    "                        },\n",
    "                        \"required\": [\"query\"]\n",
    "                    }\n",
    "                }\n",
    "            }]\n",
    "        )\n",
    "        os.environ[\"REAL_LIFE_EXAMPLES_OPENAI_ASSISTANT_ID\"] = assistant.id\n",
    "        print(\"Stored this assistant id into ENV for this session. Please change permanently in your .env file if you want to use this assistant again.\")\n",
    "        return assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thread: Thread(id='thread_9dAAwdwR2tqMIP0moeqWu5Id', created_at=1703943574, metadata={}, object='thread')\n"
     ]
    }
   ],
   "source": [
    "assistant_prompt_instruction=load_prompt(\"real-life-examples\")\n",
    "assistant = get_assistant(assistant_prompt_instruction)\n",
    "\n",
    "# Create a thread\n",
    "thread = client.beta.threads.create()\n",
    "print(f\"Thread: {thread}\")\n",
    "\n",
    "# Create a message\n",
    "technology_name = \"Firebase\"\n",
    "\n",
    "message = client.beta.threads.messages.create(\n",
    "    thread_id=thread.id,\n",
    "    role=\"user\",\n",
    "    content=technology_name,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run ID: run_hRwuUQE4mE3k7OUTla0nYEct\n",
      "Current run status: in_progress\n",
      "Current run status: requires_action\n",
      "Current run status: in_progress\n",
      "Current run status: in_progress\n",
      "Current run status: in_progress\n",
      "Current run status: in_progress\n",
      "Current run status: in_progress\n",
      "Current run status: in_progress\n",
      "Current run status: in_progress\n",
      "Current run status: in_progress\n",
      "Current run status: completed\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Real Life Examples\n",
       "\n",
       "### Firebase Functions Samples:\n",
       "\n",
       "- Description: A collection of samples demonstrating typical uses of Cloud Functions for Firebase, with various use cases including sending alerts, syncing with BigQuery, and implementing webhooks on database writes.\n",
       "- URL: https://github.com/firebase/functions-samples\n",
       "\n",
       "### AngularFire:\n",
       "\n",
       "- Description: An extension of Firebase to integrate seamlessly with Angular applications, providing developer-friendly methods to make the most of Firebase in an Angular project.\n",
       "- URL: https://github.com/angular/angularfire\n",
       "\n",
       "### Angular 16 Firebase CRUD:\n",
       "\n",
       "- Description: An application showcasing how to build CRUD operations in an Angular 16 application using Firebase Realtime Database with AngularFireDatabase service.\n",
       "- URL: https://github.com/bezkoder/angular-16-firebase-crud"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a run\n",
    "run = client.beta.threads.runs.create(\n",
    "    thread_id=thread.id,\n",
    "    assistant_id=assistant.id,\n",
    ")\n",
    "print(f\"Run ID: {run.id}\")\n",
    "\n",
    "# Wait for run to complete\n",
    "run = wait_for_run_completion(thread.id, run.id)\n",
    "\n",
    "if run.status == 'failed':\n",
    "    print(run.error)\n",
    "    exit\n",
    "elif run.status == 'requires_action':\n",
    "    run = submit_tool_outputs(thread.id, run.id, run.required_action.submit_tool_outputs.tool_calls)\n",
    "    run = wait_for_run_completion(thread.id, run.id)\n",
    "\n",
    "# Print messages from the thread\n",
    "real_life_examples = get_assistant_response(thread.id)\n",
    "display(Markdown(real_life_examples))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flash-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
